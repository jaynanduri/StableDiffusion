import os

import torch
from . import config
from tqdm import tqdm
from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler
import base64
import torchvision.transforms as T
from io import BytesIO
from transformers import CLIPTextModel, CLIPTokenizer


class StableDiffusion:
    def __init__(self):
        self.vae = AutoencoderKL.from_pretrained(config.VAE_MODEL_PATH, subfolder="vae")
        self.unet = UNet2DConditionModel.from_pretrained(config.UNET_PATH, subfolder="unet")
        self.tokenizer = CLIPTokenizer.from_pretrained(config.CLIP_TOKENIZER_PATH)
        self.text_encoder = CLIPTextModel.from_pretrained(config.CLIP_ENCODER_PATH)
        self.scheduler = LMSDiscreteScheduler(
            beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000
        )

    def reconstruct_image_from_latents(self, latent):
        """
        Reconstruct high resolution image from the denoised latent using variational auto encoder
        :param latent: lower dimensional representation of an image
        :return: high resolution PIL image object
        """
        # match the size of the input to the format accepted by decoder
        scale_latent = latent * (1 / 0.19)
        with torch.no_grad():
            decoded_img = self.vae.decode(scale_latent).sample
        transform = T.ToPILImage()
        img = transform(decoded_img[0])
        return img

    def encode_text(self, text, max_length=None):
        """
        Encode the input prompted by the user used to generate Image. These text are converted to encodings based CLIP
        Text encoder. CLIP text encoder is trained on latent images and text, therefore, these encodings represent an
        image in the latent space.
        :param text: input text entered by the user
        :param max_length: max length for tokenization
        :return: text encodings to generate image
        """
        max_length = max_length or self.tokenizer.model_max_length

        tokens = self.tokenizer(text, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
        encodings = self.text_encoder(tokens.input_ids.to(config.DEVICE))[0].to(torch.float16)

        return encodings

    def generate(self, prompt, guidance_scale=10, random_state=42, time_steps=70, resolution=512, save_steps=False):
        """
        This is the main method of the class, it takes an input from the user and converts the text into an image. This
        process involves passing the text through CLIP text encoder to generate encodings that are machine
        understandable. In a diffusion process, an image is generated by gradually denoising the image, a model is
        trained to learn the noise and the noise is subtracted at each time step. We use a scheduler to introduce to the
        image. Initially, the noise added to image is high and then over the time steps the noise is gradually reduced.
        The model predicts the noise using cross-attention layers with the text encoder, instead of learning something
        blindly model learns the noise relevant to produce the image according to the prompt. UNet architecture is used
        to predict the noise. Finally, we use a Decoder from Variational AutoEncoder to convert latent image into a
        high resolution image.
        :param prompt: text entered by user
        :param guidance_scale: used to convert the scale of the image
        :param random_state: random seed
        :param time_steps: Number of time steps
        :param resolution: resolution of the generated image
        :param save_steps: boolean to save the intermediate steps in diffusion process
        :return: latent image is reconstructed to an image of specified resolution
        """
        torch.manual_seed(random_state)
        encoded_text = self.encode_text(prompt)
        # This part can be thought of as Bias in a model or the intercept term
        bias_text = self.encode_text("", encoded_text.shape[1])
        embeddings = torch.cat([bias_text, encoded_text])
        # Initialising random noise to start diffusion process
        init_latent = torch.randn([1, self.unet.in_channels, resolution // 8, resolution // 8])
        self.scheduler.set_timesteps(time_steps)
        # Step1 - Adding noise to latents
        initialise_latent = init_latent.to(config.DEVICE).half() * self.scheduler.init_noise_sigma
        # Iterative process of diffusion, learning the noise of the latents and removing the noise from them.
        for idx, time_step in enumerate(tqdm(self.scheduler.timesteps)):
            # converting input latents to match the variance of the model
            scaled_latents = self.scheduler.scale_model_input(torch.cat([init_latent] * 2).to(torch.float16), time_step)
            with torch.no_grad():
                # noise_up represents the image with added noise, noise_guided is the noise prediction made by the model
                noise_up, noise_guided = self.unet(
                    scaled_latents.float(), time_step.float(), encoder_hidden_states=embeddings.float()
                ).sample.chunk(2)
            # denoising step - prediction = image_with_noise + scale * (image_with_noise - noise_guided)
            noise_pred = noise_up + guidance_scale * (noise_up - noise_guided)
            init_latent = self.scheduler.step(noise_pred, time_step, init_latent).prev_sample
            if save_steps:
                if not os.path.exists("./seq_latents"):
                    os.mkdir("./seq_latents")
                torch.save(init_latent, f"{time_step}_latent.pt")
        return self.reconstruct_image_from_latents(init_latent)


if __name__ == "__main__":
    diffuser = StableDiffusion()
    image = diffuser.generate("tom drinking milk")
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    img_str = base64.b64encode(buffered.getvalue())
    print(img_str)
